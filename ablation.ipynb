{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.loader import define_path\n",
    "from src.dataset.trans.data import TransDataset, extract_pred_sequence\n",
    "from src.dataset.trans.jaad_trans import JaadTransDataset\n",
    "from src.dataset.intention.jaad_dataset import build_pedb_dataset_jaad, subsample_and_balance\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.dataset.loader import PaddedSequenceDataset, IntentionSequenceDataset\n",
    "from src.transform.preprocess import ImageTransform, Compose, CropBox, ResizeFrame\n",
    "import torchvision\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import average_precision_score, f1_score, classification_report\n",
    "from train_hybrid import unpack_batch\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_paths, image_dir = define_path(use_jaad=True, use_pie=False, use_titan=False)\n",
    "anns_paths_val, image_dir_val = define_path(use_jaad=True, use_pie=False, use_titan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    encoder_type: str = 'CC'\n",
    "    encoder_pretrained: bool = False\n",
    "    epochs: int = 1\n",
    "    lr: float = 0.001\n",
    "    wd: float = 0.0\n",
    "    batch_size: int = 4\n",
    "    max_frames: int = 10\n",
    "    pred: int = 10\n",
    "    output: str = None\n",
    "    fps: int = 5\n",
    "    seed: int = 99\n",
    "    jitter_ratio: float = -1.0\n",
    "    mobilenetsmall: bool = False\n",
    "    mobilenetbig: bool = False\n",
    "    num_workers: int = 4\n",
    "\n",
    "args = Args(num_workers=0)\n",
    "max_frames = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "JAAD:\n",
      "Total number of crosses: 4002\n",
      "Total number of non-crosses: 2196\n",
      "Filtered samples: 17\n",
      "Total number of samples before and after balancing: 3794, 3794\n"
     ]
    }
   ],
   "source": [
    "image_set = \"test\"\n",
    "\n",
    "\n",
    "intent_sequences = build_pedb_dataset_jaad(anns_paths[\"JAAD\"][\"anns\"], anns_paths[\"JAAD\"][\"split\"], image_set=image_set, fps=args.fps, prediction_frames=args.pred, verbose=True)\n",
    "balance = False if image_set == \"test\" else True\n",
    "intent_sequences_cropped = subsample_and_balance(intent_sequences, max_frames=args.max_frames, seed=args.seed, balance=balance)\n",
    "\n",
    "jitter_ratio = None if args.jitter_ratio < 0 else args.jitter_ratio\n",
    "crop_preprocess = CropBox(size=224, padding_mode='pad_resize', jitter_ratio=jitter_ratio)\n",
    "resize_preprocess = ResizeFrame(resize_ratio=0.5)\n",
    "if image_set == 'train':\n",
    "    TRANSFORM = Compose([\n",
    "        resize_preprocess,\n",
    "        ImageTransform(torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1))\n",
    "        ])\n",
    "else:\n",
    "    TRANSFORM = resize_preprocess #crop_preprocess\n",
    "\n",
    "ds = IntentionSequenceDataset(intent_sequences_cropped[:100], image_dir=image_dir, hflip_p = 0.5, preprocess=TRANSFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: [1406.0, 701.0, 1473.0, 894.0]\n",
      "after:  [703.  350.5 736.5 447. ]\n",
      "before: [1396.0, 701.0, 1468.0, 899.0]\n",
      "after:  [698.  350.5 734.  449.5]\n",
      "before: [1383.0, 702.0, 1459.0, 903.0]\n",
      "after:  [691.5 351.  729.5 451.5]\n",
      "before: [1368.0, 700.0, 1442.0, 905.0]\n",
      "after:  [684.  350.  721.  452.5]\n",
      "before: [1352.0, 698.0, 1421.0, 908.0]\n",
      "after:  [676.  349.  710.5 454. ]\n",
      "torch.Size([5, 3, 540, 960])\n",
      "curr: tensor([[703, 350, 736, 447]], dtype=torch.int32)\n",
      "curr: tensor([[698, 350, 734, 449]], dtype=torch.int32)\n",
      "curr: tensor([[691, 351, 729, 451]], dtype=torch.int32)\n",
      "curr: tensor([[684, 350, 721, 452]], dtype=torch.int32)\n",
      "curr: tensor([[676, 349, 710, 454]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "for el in ds:\n",
    "    print(el['image'].shape)\n",
    "    for img, bbox in zip(el['image'], el['bbox']):\n",
    "        x0, y0, x1, y1 = bbox\n",
    "        start_point = (int(x0), int(y0))\n",
    "        end_point = (int(x1), int(y1))\n",
    "        bbox = torch.tensor(bbox, dtype=torch.int).unsqueeze(0)\n",
    "        print('curr:', bbox)\n",
    "        img *= 255\n",
    "        img = draw_bounding_boxes(img.type(torch.uint8), bbox, width=3, colors=(0,255,0))\n",
    "        img = torchvision.transforms.ToPILImage()(img)\n",
    "        img.show()\n",
    "        #img = cv2.rectangle(np.array(img), start_point, end_point, (0, 255, 0), 0)\n",
    "        #img = img.transpose(1, 2, 0)\n",
    "        #plt.imshow(img)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.models import build_encoder_res18, DecoderRNN_IMBS\n",
    "from src.early_stopping import load_from_checkpoint\n",
    "\n",
    "CP_PATH = r'D:\\VisualStudioProgram\\CIVIL459-PedestrianIntensionDetection\\checkpoints\\flowing-sweep-1\\Decoder_IMBS_lr0.001_wd0.0001_JAAD_mf10_pred10_bs16_202305281759.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using resnet18 cnn encoder!!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "encoder_res18 = build_encoder_res18(args)\n",
    "# freeze CNN-encoder during training\n",
    "encoder_res18.freeze_backbone()\n",
    "\n",
    "decoder_lstm = DecoderRNN_IMBS(CNN_embeded_size=256, h_RNN_0=256, h_RNN_1=64, h_RNN_2=16,\n",
    "                                h_FC0_dim=128, h_FC1_dim=64, h_FC2_dim=86, drop_p=0.2).to(device)\n",
    "\n",
    "model = {'encoder': encoder_res18, 'decoder': decoder_lstm}\n",
    "load_from_checkpoint(model, CP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def abl_eval_model(loader, model, device, abl_type='none'):\n",
    "    # swith to evaluate mode\n",
    "    encoder_CNN, decoder_RNN = model['encoder'], model['decoder']\n",
    "    encoder_CNN.eval()\n",
    "    decoder_RNN.eval()\n",
    "\n",
    "    batch_size = loader.batch_size\n",
    "    n_steps = len(loader)\n",
    "\n",
    "    preds = np.zeros(n_steps * batch_size)\n",
    "    tgts = np.zeros(n_steps * batch_size)\n",
    "\n",
    "    for step, inputs in enumerate(tqdm(loader)):\n",
    "        images, seq_len, pv, scene, behavior, targets = unpack_batch(inputs, device)\n",
    "        outputs_CNN = encoder_CNN(images, seq_len)\n",
    "        if abl_type == 'CNN':\n",
    "            outputs_CNN = torch.zeros_like(outputs_CNN)\n",
    "        elif abl_type == 'scene':\n",
    "            scene = torch.zeros_like(scene)\n",
    "        elif abl_type == 'behavior':\n",
    "            behavior = torch.zeros_like(behavior)\n",
    "        elif abl_type == 'pv':\n",
    "            pv = torch.zeros_like(pv)\n",
    "        elif abl_type == 'only_scene':\n",
    "             outputs_CNN = torch.zeros_like(outputs_CNN)\n",
    "             behavior = torch.zeros_like(behavior)\n",
    "             pv = torch.zeros_like(pv)\n",
    "        outputs_RNN = decoder_RNN(xc_3d=outputs_CNN, xp_3d=pv, \n",
    "                                    xb_3d=behavior, xs_2d=scene, x_lengths=seq_len)\n",
    "        \n",
    "        preds[step * batch_size: (step + 1) * batch_size] = outputs_RNN.detach().cpu().squeeze()\n",
    "        tgts[step * batch_size: (step + 1) * batch_size] = targets.detach().cpu().squeeze()\n",
    "\n",
    "    ap_score = average_precision_score(tgts, preds)\n",
    "    best_thr = decoder_RNN.threshold\n",
    "    f1 = f1_score(tgts, preds > best_thr)\n",
    "    preds = preds > best_thr\n",
    "    print(classification_report(tgts, preds), flush=True)\n",
    "    print(f\"AP: {ap_score}, F1: {f1}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abl_eval_model(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [18:42<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.52      0.58      0.55       147\n",
      "         1.0       0.82      0.78      0.80       353\n",
      "\n",
      "    accuracy                           0.72       500\n",
      "   macro avg       0.67      0.68      0.67       500\n",
      "weighted avg       0.73      0.72      0.72       500\n",
      "\n",
      "AP: 0.8342055905865203, F1: 0.7971014492753623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "abl_eval_model(test_loader, model, device, abl_type='CNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [19:48<00:00,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.42      0.50       147\n",
      "         1.0       0.79      0.89      0.84       353\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.70      0.66      0.67       500\n",
      "weighted avg       0.74      0.75      0.74       500\n",
      "\n",
      "AP: 0.8261691528741337, F1: 0.8366533864541833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "abl_eval_model(test_loader, model, device, abl_type='pv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [18:41<00:00,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.43      0.65      0.52       147\n",
      "         1.0       0.81      0.65      0.72       353\n",
      "\n",
      "    accuracy                           0.65       500\n",
      "   macro avg       0.62      0.65      0.62       500\n",
      "weighted avg       0.70      0.65      0.66       500\n",
      "\n",
      "AP: 0.8039681742822548, F1: 0.722397476340694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "abl_eval_model(test_loader, model, device, abl_type='behavior')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [22:11<00:00, 13.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.28      1.00      0.44        28\n",
      "         1.0       0.00      0.00      0.00        72\n",
      "\n",
      "    accuracy                           0.28       100\n",
      "   macro avg       0.14      0.50      0.22       100\n",
      "weighted avg       0.08      0.28      0.12       100\n",
      "\n",
      "AP: 0.8128536151791325, F1: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "d:\\VisualStudioProgram\\CIVIL459-PedestrianIntensionDetection\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\VisualStudioProgram\\CIVIL459-PedestrianIntensionDetection\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "d:\\VisualStudioProgram\\CIVIL459-PedestrianIntensionDetection\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "abl_eval_model(test_loader, model, device, abl_type='scene')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [16:20<00:00,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.60      0.41      0.49       147\n",
      "         1.0       0.78      0.89      0.83       353\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.69      0.65      0.66       500\n",
      "weighted avg       0.73      0.75      0.73       500\n",
      "\n",
      "AP: 0.79495317200476, F1: 0.8313413014608233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "abl_eval_model(test_loader, model, device, abl_type='only_scene')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
