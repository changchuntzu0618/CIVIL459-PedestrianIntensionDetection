{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.Loader.main_loader import define_path\n",
    "from src.dataset.trans.data import TransDataset, extract_pred_sequence\n",
    "from src.dataset.trans.jaad_trans import JaadTransDataset\n",
    "from src.dataset.intention.jaad_dataset import build_pedb_dataset_jaad, subsample_and_balance\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.dataset.loader import PaddedSequenceDataset, IntentionSequenceDataset\n",
    "from src.transform.preprocess import ImageTransform, Compose, CropBox\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "anns_paths, image_dir = define_path(use_jaad=True, use_pie=False, use_titan=False)\n",
    "anns_paths_val, image_dir_val = define_path(use_jaad=True, use_pie=False, use_titan=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    encoder_type: str = 'CC'\n",
    "    encoder_pretrained: bool = False\n",
    "    epochs: int = 1\n",
    "    lr: float = 0.001\n",
    "    wd: float = 0.0\n",
    "    batch_size: int = 4\n",
    "    max_frames: int = 10\n",
    "    pred: int = 10\n",
    "    output: str = None\n",
    "    fps: int = 5\n",
    "    seed: int = 99\n",
    "    jitter_ratio: float = -1.0\n",
    "    mobilenetsmall: bool = False\n",
    "    mobilenetbig: bool = False\n",
    "    num_workers: int = 4\n",
    "\n",
    "args = Args(num_workers=0)\n",
    "max_frames = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "JAAD:\n",
      "Total number of crosses: 4002\n",
      "Total number of non-crosses: 2196\n",
      "Filtered samples: 17\n",
      "Total number of samples before and after balancing: 3794, 3794\n"
     ]
    }
   ],
   "source": [
    "image_set = \"test\"\n",
    "\n",
    "\n",
    "intent_sequences = build_pedb_dataset_jaad(anns_paths[\"JAAD\"][\"anns\"], anns_paths[\"JAAD\"][\"split\"], image_set=image_set, fps=args.fps, prediction_frames=args.pred, verbose=True)\n",
    "balance = False if image_set == \"test\" else True\n",
    "intent_sequences_cropped = subsample_and_balance(intent_sequences, max_frames=args.max_frames, seed=args.seed, balance=balance)\n",
    "\n",
    "jitter_ratio = None if args.jitter_ratio < 0 else args.jitter_ratio\n",
    "crop_preprocess = CropBox(size=224, padding_mode='pad_resize', jitter_ratio=jitter_ratio)\n",
    "if image_set == 'train':\n",
    "    TRANSFORM = Compose([\n",
    "        crop_preprocess,\n",
    "        ImageTransform(torchvision.transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1))\n",
    "        ])\n",
    "else:\n",
    "    TRANSFORM = crop_preprocess\n",
    "\n",
    "ds = IntentionSequenceDataset(intent_sequences_cropped[:500], image_dir=image_dir, hflip_p = 0.5, preprocess=TRANSFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_hybrid import eval_model\n",
    "from src.model.models import build_encoder_res18, DecoderRNN_IMBS\n",
    "from src.early_stopping import load_from_checkpoint\n",
    "\n",
    "CP_PATH = '/Users/arinaruck/Desktop/courses/CIVIL459-PedestrianIntensionDetection/checkpoints/silver-sweep-5/Decoder_IMBS_lr0.001_wd0.0001_JAAD_mf10_pred10_bs16_202305241656.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using resnet18 cnn encoder!!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "\n",
    "encoder_res18 = build_encoder_res18(args)\n",
    "# freeze CNN-encoder during training\n",
    "encoder_res18.freeze_backbone()\n",
    "\n",
    "decoder_lstm = DecoderRNN_IMBS(CNN_embeded_size=256, h_RNN_0=256, h_RNN_1=64, h_RNN_2=16,\n",
    "                                h_FC0_dim=128, h_FC1_dim=64, h_FC2_dim=86, drop_p=0.2).to(device)\n",
    "\n",
    "model = {'encoder': encoder_res18, 'decoder': decoder_lstm}\n",
    "load_from_checkpoint(model, CP_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_loader = torch.utils.data.DataLoader(ds, batch_size=1, shuffle=False, num_workers=args.num_workers, pin_memory=True)\n",
    "#eval_model(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
